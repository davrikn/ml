{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd7238",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was trained during hyperparameter tuning NeuralNetTorch... Skipping this model.\n",
      "Fitting model: LightGBMLarge ... Training model for up to 147.18s of the 655.95s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 499. Best iteration is:\n",
      "\t[498]\tvalid_set's l1: 67.5839\n",
      "\t-67.5839\t = Validation score   (-mean_absolute_error)\n",
      "\t148.91s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 498.76s of remaining time.\n",
      "\t-64.3312\t = Validation score   (-mean_absolute_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1301.93s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutoGluonTesting/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.354321\n",
      "1         0.354321\n",
      "2         0.428212\n",
      "3        83.345909\n",
      "4       364.317169\n",
      "           ...    \n",
      "1531    305.037384\n",
      "1532     86.112190\n",
      "1533      4.417305\n",
      "1534      0.280041\n",
      "1535      0.368310\n",
      "Name: pv_measurement, Length: 1536, dtype: float32\n",
      "Saved this file: tuning_HPO_30_A.csv\n",
      "Total data points: 32844\n",
      "Data points to be removed: 4248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/hansal/ml/utils.py:136: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  estimated_df = estimated_df.resample('H').mean()\n",
      "/cluster/home/hansal/ml/utils.py:145: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  test_df = test_df.resample('H').mean()\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutoGluonTesting\"\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"AutoGluonTesting/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Wed Nov 9 20:13:27 UTC 2022\n",
      "Disk Space Avail:   317064.99 GB / 618408.77 GB (51.3%)\n",
      "Train Data Rows:    24970\n",
      "Train Data Columns: 79\n",
      "Tuning Data Rows:    1087\n",
      "Tuning Data Columns: 79\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, 0.0, 103.9337, 211.75438)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    770841.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.51 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 37 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 43 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])    : 31 | ['hour_0', 'hour_1', 'hour_10', 'hour_11', 'hour_12', ...]\n",
      "\t\t('object', []) :  5 | ['month_5', 'month_6', 'month_7', 'month_8', 'month_9']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 42 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) : 37 | ['hour_0', 'hour_1', 'hour_10', 'hour_11', 'hour_12', ...]\n",
      "\t1.8s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.34 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.93s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Hyperparameter tuning model: KNeighborsUnif ... Tuning model for up to 147.11s of the 1798.06s of remaining time.\n",
      "\tNo hyperparameter search space specified for KNeighborsUnif. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "Fitted model: KNeighborsUnif ...\n",
      "\t-40.3732\t = Validation score   (-mean_absolute_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t5.55s\t = Validation runtime\n",
      "Hyperparameter tuning model: KNeighborsDist ... Tuning model for up to 147.11s of the 1792.26s of remaining time.\n",
      "\tNo hyperparameter search space specified for KNeighborsDist. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "Fitted model: KNeighborsDist ...\n",
      "\t-40.4549\t = Validation score   (-mean_absolute_error)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t4.71s\t = Validation runtime\n",
      "Hyperparameter tuning model: LightGBMXT ... Tuning model for up to 147.11s of the 1787.23s of remaining time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd2996cca6544a3a36ead5105dd5103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 264. Best iteration is:\n",
      "\t[264]\tvalid_set's l1: 20.5125\n",
      "\tStopping HPO to satisfy time limit...\n",
      "Fitted model: LightGBMXT/T1 ...\n",
      "\t-3.1978\t = Validation score   (-mean_absolute_error)\n",
      "\t21.16s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitted model: LightGBMXT/T2 ...\n",
      "\t-2.9246\t = Validation score   (-mean_absolute_error)\n",
      "\t45.26s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitted model: LightGBMXT/T3 ...\n",
      "\t-3.0433\t = Validation score   (-mean_absolute_error)\n",
      "\t31.57s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitted model: LightGBMXT/T4 ...\n",
      "\t-20.5125\t = Validation score   (-mean_absolute_error)\n",
      "\t43.52s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Hyperparameter tuning model: LightGBM ... Tuning model for up to 147.11s of the 1641.61s of remaining time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6660f6b42e4d2e828b0a00ae4e81f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "import utils\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from autogluon.common import space\n",
    "\n",
    "\n",
    "def do_prediction(location, limit, name, percentage, trials):\n",
    "    x_train, tuning_data, x_test = utils.preprocess_category_estimated_observed(location)\n",
    "    x_train.drop([\"time\", 'date_forecast'], axis=1, inplace=True)\n",
    "    tuning_data.drop([\"time\", 'date_forecast'], axis=1, inplace=True)\n",
    "    x_test_date_forecast = x_test['date_forecast']\n",
    "    x_test.drop(['date_forecast'], axis=1, inplace=True)\n",
    "    \n",
    "    x_test.fillna(0, inplace=True)\n",
    "\n",
    "    label = 'pv_measurement'\n",
    "    train_data = TabularDataset(x_train)\n",
    "    \n",
    "    precentage_tuning = percentage/100\n",
    "    \n",
    "    tuning_data = TabularDataset(tuning_data)\n",
    "    thirty_percent_index = int(len(tuning_data) * precentage_tuning)\n",
    "    tuning_data = tuning_data.iloc[:thirty_percent_index]\n",
    "\n",
    "    test_data = TabularDataset(x_test)\n",
    "\n",
    "    predictor = TabularPredictor(label=label,\n",
    "                                 path=\"AutoGluonTesting\",\n",
    "                                 eval_metric='mean_absolute_error')\n",
    "    \n",
    "    num_trials = trials \n",
    "    search_strategy = 'auto'\n",
    "\n",
    "    hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "        'num_trials': num_trials,\n",
    "        'scheduler': 'local',\n",
    "        'searcher': search_strategy,\n",
    "    }\n",
    "\n",
    "    predictor.fit(train_data,\n",
    "                  time_limit=limit,\n",
    "                  tuning_data=tuning_data,\n",
    "                  hyperparameter_tune_kwargs=hyperparameter_tune_kwargs, )\n",
    "\n",
    "    y_pred = predictor.predict(test_data)\n",
    "\n",
    "    print(y_pred)\n",
    "    preds = pd.DataFrame()\n",
    "    preds['date_forecast'] = x_test_date_forecast\n",
    "    preds['predicted'] = np.asarray(y_pred)\n",
    "    preds.to_csv(str(percentage) + name + str(trials) + \"_\" +  '_' + location + '.csv')\n",
    "    print('Saved this file: ' + name +'_'+ str(percentage) + '_' + location + '.csv')\n",
    "\n",
    "    \n",
    "time_limit = 30 * 60\n",
    "percentage = 30\n",
    "trials = 20 + 20\n",
    "name= \"tuning_running_HPO\"\n",
    "print('Starting run with percentage tuning= ' + str(percentage))\n",
    "do_prediction('A', time_limit, name, percentage, trials)\n",
    "do_prediction('B', time_limit, name, percentage, trials)\n",
    "do_prediction('C', time_limit, name, percentage, trials)\n",
    "print('Done with run with percentage tuning= ' + str(percentage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1310b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
