{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cd7238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run with percentage tuning= 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/hansal/ml/utils.py:132: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  estimated_df = estimated_df.resample('H').mean()\n",
      "/cluster/home/hansal/ml/utils.py:137: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  test_df = test_df.resample('H').mean()\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutoGluonTesting\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 19800.0s\n",
      "AutoGluon will save models to \"AutoGluonTesting/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Wed Nov 9 20:13:27 UTC 2022\n",
      "Disk Space Avail:   315221.20 GB / 618408.77 GB (51.0%)\n",
      "Train Data Rows:    29667\n",
      "Train Data Columns: 43\n",
      "Tuning Data Rows:    1325\n",
      "Tuning Data Columns: 43\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 674.14552, 1195.53172)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1386013.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.33 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 34085\n",
      "Data points to be removed: 0\n",
      "1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 43 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 42 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['snow_density:kgm3']\n",
      "\t0.1s = Fit runtime\n",
      "\t43 features in original data used to generate 43 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.24 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 13196.6s of the 19799.85s of remaining time.\n",
      "\t-182.1136\t = Validation score   (-mean_absolute_error)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 13194.75s of the 19798.0s of remaining time.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_648418/1765239786.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"best_quality_random_seed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting run with percentage tuning= '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mdo_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mdo_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mdo_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_648418/1765239786.py\u001b[0m in \u001b[0;36mdo_prediction\u001b[0;34m(location, limit, name, percentage, trials)\u001b[0m\n\u001b[1;32m     40\u001b[0m     }\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     predictor.fit(train_data,\n\u001b[0m\u001b[1;32m     43\u001b[0m                   \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                   \u001b[0mtuning_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuning_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/utils/decorators.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mother_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0maux_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fit_weighted_ensemble\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save predictor to disk to enable prediction and training after interrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         self._learner.fit(\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuning_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/tabular/learner/abstract_learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Learner is already fit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_fit_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     def _fit(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/tabular/learner/default_learner.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         trainer.fit(\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/tabular/trainer/auto_trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         self._train_multi_and_ensemble(\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_rows_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_cols_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m         model_names_fit = self.train_multi_levels(\n\u001b[0m\u001b[1;32m   2372\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36mtrain_multi_levels\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mcore_kwargs_level\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_limit\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore_kwargs_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_limit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit_core\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0maux_kwargs_level\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_limit\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux_kwargs_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_limit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit_aux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             base_model_names, aux_models = self.stack_new_level(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36mstack_new_level\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mcore_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_suffix\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name_suffix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0maux_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_suffix\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name_suffix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         core_models = self.stack_new_level_core(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36mstack_new_level_core\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;31m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         return self._train_multi(\n\u001b[0m\u001b[1;32m    674\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_multi\u001b[0;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_repeat_start\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m             \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2321\u001b[0;31m             model_names_trained = self._train_multi_initial(\n\u001b[0m\u001b[1;32m   2322\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_multi_initial\u001b[0;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2169\u001b[0m             \u001b[0mtime_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhpo_time_ratio\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhpo_enabled\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2170\u001b[0;31m             models = self._train_multi_fold(\n\u001b[0m\u001b[1;32m   2171\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2172\u001b[0m                 \u001b[0mhyperparameter_tune_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameter_tune_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_multi_fold\u001b[0;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m                     \u001b[0mtime_start_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2277\u001b[0m                     \u001b[0mtime_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_limit\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime_start_model\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2278\u001b[0;31m             model_name_trained_lst = self._train_single_full(\n\u001b[0m\u001b[1;32m   2279\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameter_tune_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameter_tune_kwargs_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2280\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_single_full\u001b[0;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[1;32m   2049\u001b[0m                 )\n\u001b[1;32m   2050\u001b[0m                 \u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbagged_model_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2051\u001b[0;31m             model_names_trained = self._train_and_save(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_and_save\u001b[0;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1731\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_w_pseudo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_w_pseudo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1733\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_resources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_resources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             \u001b[0mfit_end_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_single\u001b[0;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \"\"\"\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_resources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_resources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_fit_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_fit_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtime_limit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mtime_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_limit\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0msave_bag_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"save_bag_folds\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk_fold\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_child_oof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_child_oof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_oof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_skip_oof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\u001b[0m in \u001b[0;36m_fit_single\u001b[0;34m(self, X, y, model_base, use_child_oof, time_limit, skip_oof, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0mtime_start_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodel_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"valid_oof\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oof_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_oof_pred_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                     logger.warning(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/tabular/models/knn/knn_model.py\u001b[0m in \u001b[0;36mget_oof_pred_proba\u001b[0;34m(self, X, normalize, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_oof_pred_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m\"\"\"X should be the same X passed to `.fit`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0my_oof_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_oof_pred_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_pred_probas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/tabular/models/knn/knn_model.py\u001b[0m in \u001b[0;36m_get_oof_pred_proba\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0my_oof_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifierLOOMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba_loo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0my_oof_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsRegressorLOOMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_loo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0my_oof_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_proba_to_unified_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_oof_pred_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X_unused_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/autogluon/tabular/models/knn/_knn_loo_variants.py\u001b[0m in \u001b[0;36mpredict_loo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mTarget\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \"\"\"\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    822\u001b[0m         )\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_pairwise_distances_reductions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             results = ArgKmin.compute(\n\u001b[0m\u001b[1;32m    825\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             return ArgKmin32.compute(\n\u001b[0m\u001b[1;32m    290\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\u001b[0m in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/threadpoolctl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_original_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "import utils\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from autogluon.common import space\n",
    "\n",
    "\n",
    "def do_prediction(location, limit, name, percentage, trials):\n",
    "    x_train, tuning_data, x_test = utils.preprocess_category_estimated_observed(location)\n",
    "    x_train.drop([\"time\", 'date_forecast'], axis=1, inplace=True)\n",
    "    tuning_data.drop([\"time\", 'date_forecast'], axis=1, inplace=True)\n",
    "    x_test_date_forecast = x_test['date_forecast']\n",
    "    x_test.drop(['date_forecast'], axis=1, inplace=True)\n",
    "    \n",
    "    x_test.fillna(0, inplace=True)\n",
    "\n",
    "    label = 'pv_measurement'\n",
    "    train_data = TabularDataset(x_train)\n",
    "    \n",
    "    precentage_tuning = percentage/100\n",
    "    \n",
    "    tuning_data = TabularDataset(tuning_data)\n",
    "    tuning_data = tuning_data.sample(frac=0.3, random_state=79)\n",
    "\n",
    "    test_data = TabularDataset(x_test)\n",
    "\n",
    "    predictor = TabularPredictor(label=label,\n",
    "                                 path=\"AutoGluonTesting\",\n",
    "                                 eval_metric='mean_absolute_error')\n",
    "    \n",
    "    num_trials = trials \n",
    "    search_strategy = 'auto'\n",
    "\n",
    "    hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "        'num_trials': num_trials,\n",
    "        'scheduler': 'local',\n",
    "        'searcher': search_strategy,\n",
    "    }\n",
    "\n",
    "    predictor.fit(train_data,\n",
    "                  time_limit=limit,\n",
    "                  tuning_data=tuning_data,\n",
    "                  use_bag_holdout=True,\n",
    "                  presets=['best_quality'], )\n",
    "\n",
    "    y_pred = predictor.predict(test_data)\n",
    "\n",
    "    print(y_pred)\n",
    "    preds = pd.DataFrame()\n",
    "    preds['date_forecast'] = x_test_date_forecast\n",
    "    preds['predicted'] = np.asarray(y_pred)\n",
    "    preds.to_csv(name +  '_' + location + '.csv')\n",
    "    print('Saved this file: ' + name +'_'+ str(percentage) + '_' + location + '.csv')\n",
    "\n",
    "    \n",
    "time_limit = 60 * 60 * 5.5\n",
    "percentage = 30\n",
    "trials = 20 + 20\n",
    "name= \"best_quality_random_seed\"\n",
    "print('Starting run with percentage tuning= ' + str(percentage))\n",
    "do_prediction('A', time_limit, name, percentage, trials)\n",
    "do_prediction('B', time_limit, name, percentage, trials)\n",
    "do_prediction('C', time_limit, name, percentage, trials)\n",
    "print('Done with run with percentage tuning= ' + str(percentage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1310b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "import utils\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from autogluon.common import space\n",
    "\n",
    "\n",
    "file_name = 'best_quality_random_seed_'\n",
    "\n",
    "pred_a = pd.read_csv(file_name + 'A.csv')\n",
    "pred_a['date'] = pd.to_datetime(pred_a['date_forecast'])\n",
    "pred_b = pd.read_csv(file_name + 'B.csv')\n",
    "pred_b['date'] = pd.to_datetime(pred_b['date_forecast'])\n",
    "pred_c = pd.read_csv(file_name + 'C.csv')\n",
    "pred_c['date'] = pd.to_datetime(pred_c['date_forecast'])\n",
    "\n",
    "test = pd.read_csv('data/test.csv')\n",
    "test['time'] = pd.to_datetime(test['time'])\n",
    "\n",
    "submission = pd.DataFrame(columns=['prediction'])\n",
    "\n",
    "for val in pred_a['date']:\n",
    "    if test['time'].eq(val).any():\n",
    "        row = pred_a.loc[pred_a['date'] == val]\n",
    "        submission = submission._append({'prediction': float(row['predicted'])}, ignore_index=True)\n",
    "\n",
    "for val in pred_b['date']:\n",
    "    if test['time'].eq(val).any():\n",
    "        row = pred_b.loc[pred_a['date'] == val]\n",
    "        submission = submission._append({'prediction': float(row['predicted'])}, ignore_index=True)\n",
    "\n",
    "for val in pred_c['date']:\n",
    "    if test['time'].eq(val).any():\n",
    "        row = pred_c.loc[pred_a['date'] == val]\n",
    "        submission = submission._append({'prediction': float(row['predicted'])}, ignore_index=True)\n",
    "\n",
    "submission['prediction'] = submission['prediction'].where(submission['prediction'] >= 0, 0)\n",
    "submission.index.name = \"id\"\n",
    "\n",
    "submission.to_csv('submission_' + file_name + 'attempt.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396a46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def catboost_pred(x_train, y_train, x_val, y_val, x_test, iterations, depth, seed):\n",
    "  \n",
    "    # Initialize CatBoostRegressor\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=iterations,\n",
    "        depth=depth,\n",
    "        loss_function='MAE',\n",
    "        verbose=100,\n",
    "        random_seed=seed\n",
    "    )\n",
    "\n",
    "    print(x_train.columns)\n",
    "    print(x_val.columns)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        eval_set=(x_val, y_val),\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59683205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_weather_data(csv_file, df_a, df_b, df_c):\n",
    "    # Read the CSV file into a dataframe\n",
    "    df_a = df_a.copy()\n",
    "    df_b = df_b.copy()\n",
    "    df_c = df_c.copy()\n",
    "    \n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Create a dictionary of dataframes for each location\n",
    "    dfs = {'A': df_a, 'B': df_b, 'C': df_c}\n",
    "    \n",
    "    # Filter each location's dataframe\n",
    "    filtered_dfs = {}\n",
    "    for location, df in dfs.items():\n",
    "        # Get the 'time' values from the CSV that correspond to the current location\n",
    "        times = data[data['location'] == location]['time']\n",
    "        \n",
    "        # Filter the dataframe for this location based on the times\n",
    "        # Make sure both 'date_forecast' in df and 'time' in data are of the same datetime format\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        times = pd.to_datetime(times)\n",
    "        \n",
    "        # Now, filter the dataframe based on the times\n",
    "        filtered_dfs[location] = df[df['date_forecast'].isin(times)]\n",
    "    \n",
    "    # Return the filtered dataframes\n",
    "    return filtered_dfs['A'], filtered_dfs['B'], filtered_dfs['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0ea679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_with_zero(df):\n",
    "    \"\"\"\n",
    "    Replaces missing values (NaN, None, etc.) in a DataFrame with 0.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to be processed.\n",
    "    \n",
    "    Returns:\n",
    "    - df_cleaned: The DataFrame with missing values replaced by 0.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    total_replaced = 0\n",
    "    \n",
    "    for column in df.columns:\n",
    "        missing_count = df[column].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            total_replaced += missing_count\n",
    "            df[column].fillna(0, inplace=True)\n",
    "    \n",
    "    print(f\"Total values replaced: {total_replaced}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f30188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test():\n",
    "    X_test_estimated_A = pd.read_parquet('data/A/X_test_estimated.parquet')\n",
    "    X_test_estimated_B = pd.read_parquet('data/B/X_test_estimated.parquet')\n",
    "    X_test_estimated_C = pd.read_parquet('data/C/X_test_estimated.parquet')\n",
    "    \n",
    "    X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = filter_weather_data(\"data/test.csv\", X_test_estimated_A, X_test_estimated_B, X_test_estimated_C)\n",
    "    \n",
    "    X_test_a = replace_missing_with_zero(X_test_estimated_a)\n",
    "    X_test_b = replace_missing_with_zero(X_test_estimated_b)\n",
    "    X_test_c = replace_missing_with_zero(X_test_estimated_c)\n",
    "    \n",
    "    X_test_a = encode_datetime_with_season_and_cyclical_time(X_test_a, \"date_forecast\")\n",
    "    X_test_b = encode_datetime_with_season_and_cyclical_time(X_test_b, \"date_forecast\")\n",
    "    X_test_c = encode_datetime_with_season_and_cyclical_time(X_test_c, \"date_forecast\")\n",
    "    \n",
    "    X_test_a['direct_rad_lag1'] = X_test_a['direct_rad:W'].shift(1)\n",
    "    X_test_a['diffuse_rad_lag1'] = X_test_a['diffuse_rad:W'].shift(1)\n",
    "    X_test_a['direct_rad_rolling_mean'] = X_test_a['direct_rad:W'].rolling(window=3).mean()\n",
    "    X_test_a['sun_elev_direct_rad_interaction'] = X_test_a['sun_elevation:d'] * X_test_a['direct_rad:W']\n",
    "    \n",
    "    X_test_b['direct_rad_lag1'] = X_test_b['direct_rad:W'].shift(1)\n",
    "    X_test_b['diffuse_rad_lag1'] = X_test_b['diffuse_rad:W'].shift(1)\n",
    "    X_test_b['direct_rad_rolling_mean'] = X_test_b['direct_rad:W'].rolling(window=3).mean()\n",
    "    X_test_b['sun_elev_direct_rad_interaction'] = X_test_b['sun_elevation:d'] * X_test_b['direct_rad:W']\n",
    "    \n",
    "    X_test_c['direct_rad_lag1'] = X_test_c['direct_rad:W'].shift(1)\n",
    "    X_test_c['diffuse_rad_lag1'] = X_test_c['diffuse_rad:W'].shift(1)\n",
    "    X_test_c['direct_rad_rolling_mean'] = X_test_c['direct_rad:W'].rolling(window=3).mean()\n",
    "    X_test_c['sun_elev_direct_rad_interaction'] = X_test_c['sun_elevation:d'] * X_test_c['direct_rad:W']\n",
    "\n",
    "    \n",
    "    return X_test_a.reindex(sorted(X_test_a.columns), axis=1), X_test_b.reindex(sorted(X_test_b.columns), axis=1), X_test_c.reindex(sorted(X_test_c.columns), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b8c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_datetime_with_season_and_cyclical_time(df, datetime_col):\n",
    "    \"\"\"\n",
    "    Encodes the datetime column by adding season and transforming hour into cyclical features (sine and cosine),\n",
    "    ensuring all possible categories are present and in a consistent order.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - datetime_col: The name of the datetime column in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - df_encoded: DataFrame with the datetime column encoded with seasons and cyclical hours.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Extract month and hour from the datetime column\n",
    "    df['month'] = df[datetime_col].dt.month\n",
    "    df['hour'] = df[datetime_col].dt.hour\n",
    "    \n",
    "    # Function to map month to season\n",
    "    def map_month_to_season(month):\n",
    "        if month in [3, 4, 5]:\n",
    "            return 'spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'summer'\n",
    "        elif month in [9, 10, 11]:\n",
    "            return 'autumn'\n",
    "        else:\n",
    "            return 'winter'\n",
    "\n",
    "    # Apply function to create season column\n",
    "    df['season'] = df['month'].apply(map_month_to_season)\n",
    "\n",
    "    # One-hot encode the season\n",
    "    df_encoded = pd.get_dummies(df, columns=['season'])\n",
    "\n",
    "    # Ensure all seasons are present\n",
    "    for season in ['spring', 'summer', 'autumn', 'winter']:\n",
    "        if f'season_{season}' not in df_encoded.columns:\n",
    "            df_encoded[f'season_{season}'] = 0\n",
    "\n",
    "    # Cyclical transformation of hour\n",
    "    df_encoded['hour_sin'] = np.sin(df_encoded['hour'] * (2 * np.pi / 24))\n",
    "    df_encoded['hour_cos'] = np.cos(df_encoded['hour'] * (2 * np.pi / 24))\n",
    "\n",
    "    # Drop the original month and hour columns\n",
    "    df_encoded = df_encoded.drop(['month', 'hour'], axis=1)\n",
    "\n",
    "    # Define the expected column order and reorder the dataframe columns\n",
    "    expected_columns = ['season_spring', 'season_summer', 'season_autumn', 'season_winter', \n",
    "                        'hour_sin', 'hour_cos'] + [col for col in df_encoded if col not in ['season_spring', 'season_summer', 'season_autumn', 'season_winter', 'hour_sin', 'hour_cos']]\n",
    "    df_encoded = df_encoded[expected_columns]\n",
    "\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d813171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "def apply_quantile_transformation(df1, df2, df3, feature_names):\n",
    "    \"\"\"\n",
    "    Applies quantile transformation to specified features across three dataframes and replaces the original features.\n",
    "\n",
    "    Parameters:\n",
    "    - df1, df2, df3: DataFrames to transform.\n",
    "    - feature_names: List of the names of the features to transform.\n",
    "\n",
    "    Returns:\n",
    "    - Three DataFrames with the transformed features replacing the original features.\n",
    "    \"\"\"\n",
    "    for feature_name in feature_names:\n",
    "        # Combine the feature data from all dataframes for each feature\n",
    "        combined_feature_data = pd.concat([df1[feature_name], df2[feature_name], df3[feature_name]], ignore_index=True)\n",
    "\n",
    "        # Instantiate and fit the QuantileTransformer\n",
    "        transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "        transformer.fit(combined_feature_data.to_frame())\n",
    "\n",
    "        # Transform the feature in each dataframe and replace the original feature\n",
    "        df1[feature_name] = transformer.transform(df1[[feature_name]])\n",
    "        df2[feature_name] = transformer.transform(df2[[feature_name]])\n",
    "        df3[feature_name] = transformer.transform(df3[[feature_name]])\n",
    "\n",
    "    return df1, df2, df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb952262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def write_predictions_to_csv(array1, array2, array3, filename=\"predictions.csv\"):\n",
    "    \"\"\"\n",
    "    Writes three 1D numpy arrays to a CSV file with an \"id\" and \"prediction\" column.\n",
    "    \n",
    "    :param array1: The first 1D numpy array containing the predictions.\n",
    "    :param array2: The second 1D numpy array containing the predictions.\n",
    "    :param array3: The third 1D numpy array containing the predictions.\n",
    "    :param filename: The name of the CSV file to write to.\n",
    "    \"\"\"\n",
    "    # Concatenate the arrays\n",
    "    predictions = np.concatenate((array1, array2, array3))\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header\n",
    "        writer.writerow([\"id\", \"prediction\"])\n",
    "        # Write the id and prediction\n",
    "        for idx, prediction in enumerate(predictions):\n",
    "            writer.writerow([idx, prediction])\n",
    "            \n",
    "    print(f\"File '{filename}' has been written with {len(predictions)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "104a38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_negative_values_to_zero(csv_file_path, output_csv_file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Set negative values to zero\n",
    "    df[df < 0] = 0\n",
    "\n",
    "    # Save the modified DataFrame back to a new CSV file\n",
    "    df.to_csv(output_csv_file_path, index=False)\n",
    "    print(f\"Modified CSV saved to {output_csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12034eb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 34085\n",
      "Data points to be removed: 0\n",
      "Total values replaced: 38077\n",
      "Total values replaced: 5689\n",
      "Total values replaced: 1037\n",
      "Total values replaced: 1017\n",
      "Total values replaced: 1074\n",
      "Index(['season_spring', 'season_summer', 'season_autumn', 'season_winter',\n",
      "       'hour_sin', 'hour_cos', 'absolute_humidity_2m:gm3',\n",
      "       'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
      "       'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
      "       'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
      "       'direct_rad_1h:J', 'effective_cloud_cover:p', 'fresh_snow_12h:cm',\n",
      "       'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm',\n",
      "       'fresh_snow_6h:cm', 'is_day:idx', 'is_in_shadow:idx',\n",
      "       'msl_pressure:hPa', 'precip_5min:mm', 'precip_type_5min:idx',\n",
      "       'pressure_100m:hPa', 'pressure_50m:hPa', 'prob_rime:p',\n",
      "       'rain_water:kgm2', 'relative_humidity_1000hPa:p', 'sfc_pressure:hPa',\n",
      "       'snow_depth:cm', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
      "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
      "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
      "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
      "       'wind_speed_w_1000hPa:ms', 'direct_rad_lag1', 'diffuse_rad_lag1',\n",
      "       'direct_rad_rolling_mean', 'sun_elev_direct_rad_interaction',\n",
      "       'observed'],\n",
      "      dtype='object')\n",
      "Index(['season_spring', 'season_summer', 'season_autumn', 'season_winter',\n",
      "       'hour_sin', 'hour_cos', 'absolute_humidity_2m:gm3',\n",
      "       'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
      "       'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
      "       'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
      "       'direct_rad_1h:J', 'effective_cloud_cover:p', 'fresh_snow_12h:cm',\n",
      "       'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm',\n",
      "       'fresh_snow_6h:cm', 'is_day:idx', 'is_in_shadow:idx',\n",
      "       'msl_pressure:hPa', 'precip_5min:mm', 'precip_type_5min:idx',\n",
      "       'pressure_100m:hPa', 'pressure_50m:hPa', 'prob_rime:p',\n",
      "       'rain_water:kgm2', 'relative_humidity_1000hPa:p', 'sfc_pressure:hPa',\n",
      "       'snow_depth:cm', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
      "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
      "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
      "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
      "       'wind_speed_w_1000hPa:ms', 'direct_rad_lag1', 'diffuse_rad_lag1',\n",
      "       'direct_rad_rolling_mean', 'sun_elev_direct_rad_interaction',\n",
      "       'observed'],\n",
      "      dtype='object')\n",
      "0:\tlearn: 556.8018370\ttest: 1192.8506488\tbest: 1192.8506488 (0)\ttotal: 644ms\tremaining: 5m 21s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_784459/1419397926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"cat_boost_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_cat_boost_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# denne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_cat_boost_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# denne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_cat_boost_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#DENNE MÅ TESTES MER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_784459/1419397926.py\u001b[0m in \u001b[0;36mmake_cat_boost_pred\u001b[0;34m(iterations, depth, seed)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_cat_boost_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_boost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_boost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_boost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_784459/1419397926.py\u001b[0m in \u001b[0;36mcat_boost\u001b[0;34m(location, iterations, depth, seed)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatboost_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_784459/2975313687.py\u001b[0m in \u001b[0;36mcatboost_pred\u001b[0;34m(x_train, y_train, x_val, y_val, x_test, iterations, depth, seed)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5701\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5703\u001b[0;31m         return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[1;32m   5704\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5705\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2317\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_cout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_cerr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m             \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2319\u001b[0;31m             self._train(\n\u001b[0m\u001b[1;32m   2320\u001b[0m                 \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2321\u001b[0m                 \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def cat_boost(location, iterations, depth, seed):\n",
    "    x_train, x_test, x_observed = utils.preprocess_category_estimated_observed(location)\n",
    "    x_train.drop([\"time\", \"snow_density:kgm3\"], axis=1, inplace=True)\n",
    "    x_observed.drop([\"time\", \"date_calc\", \"snow_density:kgm3\"], axis=1, inplace=True)\n",
    "\n",
    "    test_a, test_b, test_c = prepare_test()\n",
    "    if location == \"A\":\n",
    "        x_test = test_a\n",
    "    if location == \"B\":\n",
    "        x_test = test_b\n",
    "    if location == \"C\":\n",
    "        x_test = test_c\n",
    "        \n",
    "    x_train = encode_datetime_with_season_and_cyclical_time(x_train, \"date_forecast\")\n",
    "    x_observed = encode_datetime_with_season_and_cyclical_time(x_observed, \"date_forecast\")\n",
    "    x_test.drop([\"date_calc\", \"snow_density:kgm3\", \"date_forecast\", 'elevation:m', 'snow_drift:idx'], axis=1, inplace=True)\n",
    "\n",
    "    x_train['direct_rad_lag1'] = x_train['direct_rad:W'].shift(1)\n",
    "    x_train['diffuse_rad_lag1'] = x_train['diffuse_rad:W'].shift(1)\n",
    "    x_train['direct_rad_rolling_mean'] = x_train['direct_rad:W'].rolling(window=3).mean()\n",
    "    x_train['sun_elev_direct_rad_interaction'] = x_train['sun_elevation:d'] * x_train['direct_rad:W']\n",
    "    \n",
    "    x_observed['direct_rad_lag1'] = x_observed['direct_rad:W'].shift(1)\n",
    "    x_observed['diffuse_rad_lag1'] = x_observed['diffuse_rad:W'].shift(1)\n",
    "    x_observed['direct_rad_rolling_mean'] = x_observed['direct_rad:W'].rolling(window=3).mean()\n",
    "    x_observed['sun_elev_direct_rad_interaction'] = x_observed['sun_elevation:d'] * x_observed['direct_rad:W']\n",
    "    \n",
    "    x_train, x_observed, x_test = apply_quantile_transformation(x_train, x_observed, x_test, ['direct_rad:W', 'diffuse_rad:W', 'sun_elev_direct_rad_interaction', 'clear_sky_rad:W'])\n",
    "\n",
    "    # Filter to include only data from April to July\n",
    "    april_to_july_df = x_train[x_train['date_forecast'].dt.month.isin([4, 5, 6, 7])]\n",
    "\n",
    "    # Sample 30% of the data from this subset\n",
    "    random_sample = april_to_july_df.sample(frac=0.3, random_state=seed)\n",
    "\n",
    "    # Remove the sampled rows from the original dataframe\n",
    "    x_train = x_train.drop(random_sample.index)\n",
    "    \n",
    "    x_train.drop([\"date_forecast\", 'elevation:m', 'snow_drift:idx'], axis=1, inplace=True)\n",
    "    x_observed.drop([\"date_forecast\", 'elevation:m', 'snow_drift:idx'], axis=1, inplace=True)\n",
    "    random_sample.drop([\"date_forecast\", 'elevation:m', 'snow_drift:idx'], axis=1, inplace=True)\n",
    "\n",
    "    x_train['observed'] = 1\n",
    "    random_sample['observed'] = 1\n",
    "    x_observed['observed'] = 0\n",
    "    x_test['observed'] = 0\n",
    "    \n",
    "    x_train = pd.concat([x_train, x_observed], ignore_index=True)\n",
    "    X = x_train.drop('pv_measurement', axis=1)  # Features\n",
    "    Y = x_train['pv_measurement']  # Target\n",
    "    \n",
    "    X_val = random_sample.drop('pv_measurement', axis=1)  # Features\n",
    "    Y_val = random_sample['pv_measurement']  # Target\n",
    "\n",
    "    \n",
    "    y_pred = catboost_pred(X, Y, X_val, Y_val, x_test, iterations, depth, seed)\n",
    "\n",
    "    return np.asarray(y_pred)\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "import time\n",
    "\n",
    "def make_cat_boost_pred(iterations, depth, seed):\n",
    "    start_time = time.time()\n",
    "    a = cat_boost(\"A\", iterations, depth, seed)\n",
    "    b = cat_boost(\"B\", int(iterations/2), depth, seed)\n",
    "    c = cat_boost(\"C\", int(iterations/2), depth, seed)\n",
    "    write_predictions_to_csv(a, b, c, filename=\"cat_boost2_\"+str(depth)+\".csv\")\n",
    "    set_negative_values_to_zero(\"cat_boost2_\"+str(depth)+\".csv\", \"cat_boost2_\"+str(depth)+\".csv\")\n",
    "    print(\"\"\"\n",
    "          \n",
    "          \n",
    "          New iterations!!!!\n",
    "          \n",
    "          \n",
    "          \"\"\")\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"The code took {total_time} seconds to run.\")\n",
    "    return \"cat_boost_\"+str(depth)+\".csv\"\n",
    "\n",
    "a = make_cat_boost_pred(500, 15, 2)# denne\n",
    "b = make_cat_boost_pred(1000, 14, 12)# denne\n",
    "c = make_cat_boost_pred(2000, 13, 22) #DENNE MÅ TESTES MER\n",
    "d = make_cat_boost_pred(3000, 12, 32)\n",
    "e = make_cat_boost_pred(4000, 11, 42)\n",
    "f = make_cat_boost_pred(5000, 10, 52)\n",
    "g = make_cat_boost_pred(7000, 9, 62)\n",
    "h = make_cat_boost_pred(9000, 8, 52)\n",
    "i = make_cat_boost_pred(10000, 7, 62)\n",
    "\n",
    "\n",
    "csv_files = [a,b,c,d,e,f,g,h,i]  # Replace with your filenames\n",
    "output_file = 'catboostAv2.csv'\n",
    "average_predictions(csv_files, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66ff6d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified CSV saved to cat_boost2_14.csv\n"
     ]
    }
   ],
   "source": [
    "set_negative_values_to_zero(\"cat_boost2_\"+str(14)+\".csv\", \"cat_boost2_\"+str(14)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ceb9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7b016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
