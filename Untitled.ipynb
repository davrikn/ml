{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd7238",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was trained during hyperparameter tuning NeuralNetTorch... Skipping this model.\n",
      "Fitting model: LightGBMLarge ... Training model for up to 294.44s of the 1504.43s of remaining time.\n",
      "\t-5.1223\t = Validation score   (-mean_absolute_error)\n",
      "\t119.87s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1377.11s of remaining time.\n",
      "\t-3.8581\t = Validation score   (-mean_absolute_error)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2223.77s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutoGluonTesting/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0.006862\n",
      "1        0.007724\n",
      "2        0.008100\n",
      "3        2.155264\n",
      "4       27.009487\n",
      "          ...    \n",
      "1531    62.562881\n",
      "1532     8.083497\n",
      "1533    10.109064\n",
      "1534     0.002251\n",
      "1535     0.001569\n",
      "Name: pv_measurement, Length: 1536, dtype: float32\n",
      "Saved this file: tuning_HPO_30_C.csv\n",
      "Done with run with percentage tuning= 30\n",
      "Starting run with percentage tuning= 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/hansal/ml/utils.py:136: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  estimated_df = estimated_df.resample('H').mean()\n",
      "/cluster/home/hansal/ml/utils.py:145: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  test_df = test_df.resample('H').mean()\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutoGluonTesting\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 34085\n",
      "Data points to be removed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"AutoGluonTesting/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Wed Nov 9 20:13:27 UTC 2022\n",
      "Disk Space Avail:   317099.03 GB / 618408.77 GB (51.3%)\n",
      "Train Data Rows:    29667\n",
      "Train Data Columns: 79\n",
      "Tuning Data Rows:    1325\n",
      "Tuning Data Columns: 79\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 674.14552, 1195.53172)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    770627.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 11.31 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 37 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 43 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])    : 31 | ['hour_0', 'hour_1', 'hour_10', 'hour_11', 'hour_12', ...]\n",
      "\t\t('object', []) :  5 | ['month_5', 'month_6', 'month_7', 'month_8', 'month_9']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 42 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) : 37 | ['hour_0', 'hour_1', 'hour_10', 'hour_11', 'hour_12', ...]\n",
      "\t1.1s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.35 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.25s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Hyperparameter tuning model: KNeighborsUnif ... Tuning model for up to 294.44s of the 3598.75s of remaining time.\n",
      "\tNo hyperparameter search space specified for KNeighborsUnif. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "Fitted model: KNeighborsUnif ...\n",
      "\t-281.2254\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t5.03s\t = Validation runtime\n",
      "Hyperparameter tuning model: KNeighborsDist ... Tuning model for up to 294.44s of the 3593.46s of remaining time.\n",
      "\tNo hyperparameter search space specified for KNeighborsDist. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "Fitted model: KNeighborsDist ...\n",
      "\t-281.8238\t = Validation score   (-mean_absolute_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t5.09s\t = Validation runtime\n",
      "Hyperparameter tuning model: LightGBMXT ... Tuning model for up to 294.44s of the 3588.12s of remaining time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17237b8fada349e69d867e362770d722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 72.2881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1084. Best iteration is:\n",
      "\t[1079]\tvalid_set's l1: 72.0934\n",
      "\tStopping HPO to satisfy time limit...\n",
      "Fitted model: LightGBMXT/T1 ...\n",
      "\t-72.311\t = Validation score   (-mean_absolute_error)\n",
      "\t21.32s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitted model: LightGBMXT/T2 ...\n",
      "\t-71.3606\t = Validation score   (-mean_absolute_error)\n",
      "\t69.06s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitted model: LightGBMXT/T3 ...\n",
      "\t-70.827\t = Validation score   (-mean_absolute_error)\n",
      "\t32.64s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitted model: LightGBMXT/T4 ...\n",
      "\t-72.0934\t = Validation score   (-mean_absolute_error)\n",
      "\t165.98s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Hyperparameter tuning model: LightGBM ... Tuning model for up to 294.44s of the 3293.54s of remaining time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91da6c553c6c4f9386bea80b8faf4810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 551. Best iteration is:\n",
      "\t[551]\tvalid_set's l1: 92.048\n",
      "\tStopping HPO to satisfy time limit...\n",
      "Fitted model: LightGBM/T1 ...\n",
      "\t-70.7593\t = Validation score   (-mean_absolute_error)\n",
      "\t48.06s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitted model: LightGBM/T2 ...\n",
      "\t-67.4734\t = Validation score   (-mean_absolute_error)\n",
      "\t71.19s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitted model: LightGBM/T3 ...\n",
      "\t-71.2332\t = Validation score   (-mean_absolute_error)\n",
      "\t55.55s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitted model: LightGBM/T4 ...\n",
      "\t-92.048\t = Validation score   (-mean_absolute_error)\n",
      "\t113.74s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Hyperparameter tuning model: RandomForestMSE ... Tuning model for up to 294.44s of the 2999.37s of remaining time.\n",
      "\tNo hyperparameter search space specified for RandomForestMSE. Skipping HPO. Will train one model based on the provided hyperparameters.\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "import utils\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from autogluon.common import space\n",
    "\n",
    "\n",
    "def do_prediction(location, limit, name, percentage, trials):\n",
    "    x_train, tuning_data, x_test = utils.preprocess_category_estimated_observed(location)\n",
    "    x_train.drop([\"time\", 'date_forecast'], axis=1, inplace=True)\n",
    "    tuning_data.drop([\"time\", 'date_forecast'], axis=1, inplace=True)\n",
    "    x_test_date_forecast = x_test['date_forecast']\n",
    "    x_test.drop(['date_forecast'], axis=1, inplace=True)\n",
    "    \n",
    "    x_test.fillna(0, inplace=True)\n",
    "\n",
    "    label = 'pv_measurement'\n",
    "    train_data = TabularDataset(x_train)\n",
    "    \n",
    "    precentage_tuning = percentage/100\n",
    "    \n",
    "    tuning_data = TabularDataset(tuning_data)\n",
    "    thirty_percent_index = int(len(tuning_data) * precentage_tuning)\n",
    "    tuning_data = tuning_data.iloc[:thirty_percent_index]\n",
    "\n",
    "    test_data = TabularDataset(x_test)\n",
    "\n",
    "    predictor = TabularPredictor(label=label,\n",
    "                                 path=\"AutoGluonTesting\",\n",
    "                                 eval_metric='mean_absolute_error')\n",
    "    \n",
    "    num_trials = trials  # try at most 30 different hyperparameter configurations for each type of model\n",
    "    search_strategy = 'auto'  # to tune hyperparameters using random search routine with a local scheduler\n",
    "\n",
    "    hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "        'num_trials': num_trials,\n",
    "        'scheduler': 'local',\n",
    "        'searcher': search_strategy,\n",
    "    }\n",
    "\n",
    "    predictor.fit(train_data,\n",
    "                  time_limit=limit,\n",
    "                  tuning_data=tuning_data,\n",
    "                  hyperparameter_tune_kwargs=hyperparameter_tune_kwargs, )\n",
    "\n",
    "    y_pred = predictor.predict(test_data)\n",
    "\n",
    "    print(y_pred)\n",
    "    preds = pd.DataFrame()\n",
    "    preds['date_forecast'] = x_test_date_forecast\n",
    "    preds['predicted'] = np.asarray(y_pred)\n",
    "    preds.to_csv(str(percentage) + name + str(trials) + \"_\" +  '_' + location + '.csv')\n",
    "    print('Saved this file: ' + name +'_'+ str(percentage) + '_' + location + '.csv')\n",
    "\n",
    "for i in range(10):    \n",
    "    time_limit = 60 * 60\n",
    "    percentage = 30\n",
    "    trials = 20 + 10 * i\n",
    "    name= \"tuning_HPO\"\n",
    "    print('Starting run with percentage tuning= ' + str(percentage))\n",
    "    do_prediction('A', time_limit, name, percentage, trials)\n",
    "    do_prediction('B', time_limit, name, percentage, trials)\n",
    "    do_prediction('C', time_limit, name, percentage, trials)\n",
    "    print('Done with run with percentage tuning= ' + str(percentage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1310b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
